<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DreamVLA: Vision-Language-Action Models\\ Dream Comprehensive World Knowledge">
  <meta name="keywords" content="Semantic Orientation, Spatial Reasoning, Robotic Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge</title>
	<!-- <link rel="icon" type="image/png" href="images/sofar_icon.png"/> -->

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

    <link rel="stylesheet" href="./static/css/own.min.css">
    <link rel="stylesheet" href="./static/css/custom.min.css">
    <link href="https://fonts.googleapis.com/css?family=Noto+Sans|Shantell+Sans" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.0/css/bulma.min.css"/>
    <link href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/css/bulma-carousel.min.css" rel="stylesheet"/>
    <script defer src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/js/bulma-carousel.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/js/all.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <script defer src="./static/js/index.min.js"></script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size: 44.5px;">
              <a class="word1 shakefont" style="color: #FFC5C5">D</a><a class="word2 shakefont" style="color: #ecd1aa">r</a><a class="word3 shakefont" style="color: #fe8e8e">e</a><a class="word4 shakefont" style="color: #98d4e6">a</a><a class="word5 shakefont" style="color: #c8a4dd">m</a><a class="word6 shakefont" style="color: #f2cd90">VLA</a>:
              <a target="_blank" style="color: black">Vision-Language-Action Models Dream Comprehensive World Knowledge</a></h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zhangwenyao1.github.io/">Wenyao Zhang<sup>124*</sup></a></span>,
            <span class="author-block">
              <a href="https://qizekun.github.io/">Hongsi Liu<sup>27*</sup></a></span>,
            <span class="author-block">
              <a href="https://qizekun.github.io/">Zekun Qi<sup>34*</sup></a></span>,
            <span class="author-block">
              <a href="https://wangyunnan.github.io/">Yunan Wang<sup>12*</sup></a></span>,
            <br>
            <span class="author-block">
              <a href="https://zhangwenyao1.github.io/">Xinqiang Yu<sup>4</sup></a></span>,
            <span class="author-block">
              <a href="https://jzhzhang.github.io/">Jiazhao Zhang<sup>45</sup></a></span>,
            <span class="author-block">
              <a href="https://runpeidong.web.illinois.edu/">Runpei Dong<sup>6</sup></a></span>,
            <span class="author-block">
              <a href="https://jiaweihe.com/">Jiawei He<sup>4</sup></a></span>,
            <br>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en">Zhizheng Zhang<sup>4</sup></a></span>,
            <span class="author-block">
                <a href="https://hughw19.github.io/">He Wang<sup>4</sup></a></span>,
            <span class="author-block">
              <a href="https://ericyi.github.io/">Li Yi<sup>3</sup></a></span>,
            <span class="author-block">
                <a href="https://www.eitech.edu.cn/?p=leader-Wenjun%20Zeng&tid=19&lang=en">Wenjun Zeng<sup>2</sup></a></span>,
            <span class="author-block">
              <a href="http://home.ustc.edu.cn/~jinxustc/">Xin Jin<sup>2‚Ä†</sup></a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">* equal contribution</span>
            <span class="author-block">‚Ä† corresponding authors</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University</span>
            <span class="author-block"><sup>2</sup>Eastern Institute of Technology</span>
            <span class="author-block"><sup>3</sup>Tsinghua University</span>
          <br>
            <span class="author-block"><sup>4</sup>Galbot</span>
            <span class="author-block"><sup>5</sup>Peking University</span>
            <span class="author-block"><sup>6</sup>UIUC</span>
            <span class="author-block"><sup>7</sup>University of Science and Technology of China</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">

<!--              <span class="link-block">-->
<!--                <a target="_blank" href="sofar.pdf"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.13143"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/Zhangwenyao1/DreamVLA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/WenyaoZhang/DreamVLA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-face-smiling-hands"></i>
                  </span>
                  <span>Huggingface</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<style>
  .video-container {
    position: relative;
    width: 85%;
  }

  video {
    width: 100%;
    border-radius: 10px;
    box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.2);
  }

  .control-btn {
    position: absolute;
    background: rgba(0, 0, 0, 0.6);
    color: white;
    border: none;
    border-radius: 50%;
    width: 40px;
    height: 40px;
    font-size: 1.2em;
    display: flex;
    align-items: center;
    justify-content: center;
    cursor: pointer;
    transition: background 0.3s ease;
  }

  .control-btn:hover {
    background: rgba(0, 0, 0, 0.8);
  }

  .sound-btn {
    bottom: 10px;
    right: 10px;
  }

  .play-btn {
    bottom: 10px;
    left: 10px;
  }
</style>

<!-- <script>
  let video = document.getElementById("teaser");
  let soundBtn = document.getElementById("toggleSound");
  let playBtn = document.getElementById("togglePlay");

  soundBtn.addEventListener("click", function () {
    video.muted = !video.muted;
    this.innerHTML = video.muted ? "üîá" : "üîä";
  });

  playBtn.addEventListener("click", function () {
    if (video.paused) {
      video.play();
      this.innerHTML = "‚è∏Ô∏è";
    } else {
      video.pause();
      this.innerHTML = "‚ñ∂Ô∏è";
    }
  });
</script> -->



<section class="section">
  <div class="container is-max-desktop">
    <div class="my-block">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified" style="text-weight: bold;">
            <p>
              Recent advances in vision-language-action (VLA) models have shown promise in integrating image generation with action prediction to improve generalization
              and reasoning in robot manipulation. However, existing methods are limited to challenging image-based forecasting, which suffers from redundant information
              and lacks comprehensive and critical world knowledge, including dynamic, spatial and semantic information. To address these limitations, we propose DreamVLA,
              a novel VLA framework that integrates comprehensive world knowledge forecasting to enable inverse dynamics modeling, thereby establishing a perception-prediction-action 
              loop for manipulation tasks. Specifically, DreamVLA introduces a dynamic-region-guided world knowledge prediction, integrated with the spatial and semantic cues, 
              which provide compact yet comprehensive representations for action planning. This design aligns with how humans interact with the world by first forming abstract multimodal reasoning chains before acting. To mitigate
              interference among the dynamic, spatial and semantic information during training,
              we adopt a block-wise structured attention mechanism that masks their mutual
              attention, preventing information leakage and keeping each representation clean
              and disentangled. Moreover, to model the conditional distribution over future
              actions, we employ a diffusion-based transformer that disentangles action represen-
              tations from shared latent features. Extensive experiments on both real-world and
              simulation environments demonstrate that DreamVLA achieves 76.7% success rate
              on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>


<!-- <section class="section">
   <div class="container is-max-desktop">
    <div class="my-block">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Highlight</h2>
          <div class="content has-text-justified" style="text-weight: bold;">
            <p>
                1. Recast the vision‚Äìlanguage‚Äìaction model as a perception‚Äìprediction‚Äìaction model and make the model explicitly predict a compact set of dynamic, spatial and high‚Äëlevel semantic information, supplying concise yet comprehensive look‚Äëahead
                cues for planning.
            </p>
            <p>
                2. Introduce a block-wise structured-attention mechanism, coupled with a diffusion-transformer decoder, to suppress representation noise from cross-type knowledge leakage and thus enable coherent multi-step action reasoning.
            </p>
            <p>
                3. Seting a new state of the art on the CALVIN ABC‚ÄëD benchmark and achieving success rate on real robot tasks, demonstrating the effectiveness of our approach in enhancing the reasoning and generalization capabilities of VLA models.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div> -->

 <!-- <div class="container is-max-desktop">-->
<!--    <div class="my-block">-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--        <div class="column is-four-fifths">-->
<!--          <h2 class="title is-3">Abstract</h2>-->
<!--          <div class="content has-text-justified">-->
<!--            <p>-->
<!--              Spatial intelligence is an indispensable part of embodied AI. Recent works endow embodied agents with limited aspects of spatial intelligence by developing the ability to perceive object locations and positional relations, overlooking precise perception of object orientation. Legging behind in this aspect renders limited robotic manipulation capabilities for numerous tasks requiring intra-object spatial understanding, e.g., stocking goods into a shelf with their logos oriented toward the customers. In this paper, we advance spatial intelligence to the next level by investigating orientational intelligence. We make the first endeavor to connect semantics to object orientation, obviating the need to describe object orientations based on references or canonical status.-->
<!--              Specifically, we build an orientation foundation model, namely PointSO, which infers direction vectors clearly associated with a specific object interaction function. To train PointSO, we curate OrienText300K, a large-scale 3D model dataset annotated with semantic direction vectors. Furthermore, we integrate PointSO into a Vision-and-Language Model (VLM) system to generate actions for robotic manipulation with both positional and orientational awareness.-->
<!--              Extensive experiments on simulation and real-world environments demonstrate that we make full-stack contributions in endowing embodied agents with groundbreaking manipulation capabilities.-->
<!--            </p>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div> -->



  <div class="container is-fullhd">
    <div class="my-block">
        <div class="column is-full-width">
          <h2 class="title is-3">Comparison with previous VLA paradigm</h2>

          <div>
            <img src="images/paradigm_compare.gif"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
          </div>

          <h2 class="content has-text-justified">

      (a) Vanilla VLA directly maps visual observations and language instructions to actions. (b) Models leveraging separate
      image/video generation or copilot models to generate future frames or trajectories, subsequently guiding an action head.
      (c) VLA variants explicitly predict a subgoal image as an intermediate visual reasoning step prior to action generation.
      (d) Our proposed DreamVLA, which explicitly predicts dynamic regions, depth map, semantics (DINOv2 and SAM)
      knowledge, significantly enhances the model's action reasoning and generalization.
          </h2>

      </div>
    </div>



  <div class="container is-fullhd">
    <div class="my-block">
      <div class="column is-full-width">
        <h2 class="title is-3">Pipeline</h2>
  
        <div>
          <img src="images/pipeline.gif" class="interpolation-image" alt="Interpolation end reference image." />
        </div>
  
        <h2 class="content has-text-justified">
        Given the current robot state, observation, and language instruction, DreamVLA encodes multimodal
        inputs via frozen text, visual encoders and a tunable state encoder.
        These tokens, together with a learnable set of <<b>dream</b>> queries, are processed by a large language model to
        produce world embedding.
        Three lightweight decoders then project each corresponding element of this embedding into the dynamics region, monocular depth and high-level semantics.
        A separate <<b>action</b>> query draws a latent action embedding, which conditions a diffusion transformer that
          refines Gaussian noise into an n-step action sequence.
          The dashed box highlights prediction heads that are used only during training, inference skips these heads and
          operates directly on the world embedding.
        </h2>
  
      </div>
    </div>



  <div class="container is-fullhd">
    <div class="my-block">
      <div class="column is-full-width">
        <h2 class="title is-3">Dynamic Regions</h2>
  
        <div>
          <img src="images/dynamic_mask.png" class="interpolation-image" alt="Interpolation end reference image." />
        </div>
  
        <h2 class="content has-text-justified">
          Visualization of dynamic regions over time.
          We show the static camera (left) and wrist-mounted camera (right) observations alongside the corresponding dynamic masks
          generated by our method at multiple time steps.
          The masks highlight dynamic regions by leveraging optical flow trajectories extracted via
          CoTracker.
          Compared to the original observations, our method effectively suppresses irrelevant background and focuses on
          interaction-relevant areas (e.g., moving objects and end-effector), enabling more structured and efficient action
          reasoning.
        </h2>
  
      </div>
    </div>

    <style>
      /* ËÆ©ÂÆΩÂ∫¶ÊúÄÂ§öÂç†ÂÆπÂô®ÁöÑ 60%ÔºåÈ´òÂ∫¶Ëá™ÈÄÇÂ∫î */
      .interpolation-image-small {
        margin-left: 20rem;           /* Ëá™Âä®Âç†ÊéâÂ∑¶‰æßÁ©∫ÁôΩ ‚Üí Âè≥Áßª */
        margin-right: 2rem; 
        max-width:40%;
        height: auto;
      }
    </style>

      <div class="container is-fullhd">
        <div class="my-block">
          <div class="column is-full-width">

            <h2 class="title is-3">Block-wise Structured Attention</h2>
      
            <div>
              <img src="images/attention.png" class="interpolation-image-small" alt="Interpolation end reference image." />
            </div>
            <h2 class="content has-text-justified">
              To preserve clear modality bound-aries, <<b>dream</b>> query is decomposed into three sub-queries (dynamic, depth and semantics). If these sub-queries could freely attend to one another,
                high-frequency flow details would contaminate depth reasoning, and semantic cues might bleed into motion features, producing noisy mixed representations. We therefore mask their mutual
                attention: each sub-query attends only to the shared visual, language, and state tokens, while direct links among the three are disabled, keeping their latent features disentangled and free of
                cross-talk.
            </h2>
      
          </div>
        </div>




      <div class="my-block">
        <div class="column is-full-width">
          <h2 class="title is-3">CALVIN ABC-D Experiments</h2>
          <h2 class="content has-text-justified">
            We present the average success computed over 1000 rollouts
            for each task and the average number of completed tasks to solve 5 instructions consecutively (Avg. Len.). DreamVLA shows significant superiority over baselines. The best results are bolde.
          </h2>
          <div>
            <img src="images/sim_exp.png" class="interpolation-image" alt="Interpolation end reference image." width="65%" style="margin-left: 10rem;"/>
          </div>
        </div>
      </div>











    <div class="my-block">
        <div class="column is-full-width">
          <h2 class="title is-3">CALVIN Demo</h2>
          <div>
          <h2 class="content has-text-justified">
            Semantic orientation can not only be applied to manipulation tasks but also to robotic navigation task.
            This orientation-aware constraint enhances the navigation process by ensuring precise alignment with the desired orientation, thereby improving task performance in scenarios where directionality is critical.
          </h2>
          <div class="rows">
            <div class="columns">
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/0.mp4' type='video/mp4'>
                </video>
                lift_pink_block_slider->place_in_slider->turn_on_led->close_drawer->rotate_blue_block_left
              </div>
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/1.mp4' type='video/mp4'>
                </video>
                turn_off_led->move_slider_left->rotate_blue_block_right->lift_red_block_table->stack_block
              </div>
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/2.mp4' type='video/mp4'>
                </video>
                turn_off_led->lift_red_block_slider->place_in_slider->move_slider_right->push_blue_block_left
              </div>
            </div>
          </div>
          <div class="rows">
            <div class="columns">
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%' ><source data-src='./videos/3.mp4' type='video/mp4'></video>
                  rotate_pink_block_right->move_slider_right->turn_on_led->close_drawer->lift_pink_block_table
              </div>
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'><source data-src='./videos/4.mp4' type='video/mp4'></video>
                rotate_pink_block_left->turn_on_led->push_into_drawer->lift_pink_block_drawer->place_in_drawer
              </div>
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/5.mp4' type='video/mp4'>
                </video>
                rotate_blue_block_right->lift_pink_block_table->place_in_slider->move_slider_left->open_drawer
              </div>
            </div>
          </div>

          <!-- <div class="rows">
            <div class="columns">
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/6.mp4' type='video/mp4'>
                </video>
              push_pink_block_rightopen_drawer->move_slider_right->lift_red_block_table->place_in_drawer
              </div>
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/7.mp4' type='video/mp4'>
                </video>
              open_drawer->turn_on_led->push_blue_block_right->move_slider_left->lift_pink_block_slider
              </div>
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/8.mp4' type='video/mp4'>
                </video>
              open_drawer->rotate_red_block_left->push_into_drawer->turn_off_led->lift_pink_block_slider
              </div>
            </div>
            </div> -->

    </div>

    <div class="my-block">
        <div class="column is-full-width">
          <h2 class="title is-3">Real-world Demo</h2>
          <div>
          <h2 class="content has-text-justified">
             The real-world demonstration showcases the practical application of our model in a physical environment.
             We collect post-training data using SoFar, a modular-based robot manipulation method.
          </h2>

          <div class="rows">
            <div class="columns">
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/grasp_bottle_0.mp4' type='video/mp4'>
                </video>
                Pick up the bottle.
              </div>
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/grasp_yellow_doll_sur.mp4' type='video/mp4'>
                </video>
                Pick up the yellow doll.
              </div>
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/grasp_white_doll_3.mp4' type='video/mp4'>
                </video>
                Pick up the white doll.
              </div>
            </div>
          </div>


          <div class="rows">
            <div class="columns">
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/place_banana_1.mp4' type='video/mp4'>
                </video>
                Place the banana into the basket.
              </div>
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/place_chili_1.mp4' type='video/mp4'>
                </video>
                Place the chili into the basket.
              </div>
              <div class="column has-text-centered">
                <video class='lazy' autoplay loop muted width='100%'>
                  <source data-src='./videos/place_chili_2.mp4' type='video/mp4'>
                </video>
                Place the chili into the white basket.
              </div>
            </div>
          </div>
    </div>


    <br>

</section>

<div>
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{sofar25,
          author       = {Zekun Qi and
                          Wenyao Zhang and
                          Yufei Ding and
                          Runpei Dong and
                          Xinqiang Yu and
                          Jingwen Li and
                          Lingyun Xu and
                          Baoyu Li and
                          Xialin He and
                          Guofan Fan and
                          Jiazhao Zhang and
                          Jiawei He and
                          Jiayuan Gu and
                          Xin Jin and
                          Kaisheng Ma and
                          Zhizheng Zhang and
                          He Wang and
                          Li Yi},
          title        = {SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and
                          Object Manipulation},
          journal      = {CoRR},
          volume       = {abs/2502.13143},
          year         = {2025},
          url          = {https://doi.org/10.48550/arXiv.2502.13143},
          doi          = {10.48550/ARXIV.2502.13143},
          eprinttype    = {arXiv},
          eprint       = {2502.13143}
        }</code></pre>
    </div>
  </div>
</div>


<footer class="footer" style="padding: 2em;">
    <div class="content has-text-centered">
      <p>
        The website template is borrowed from <a href="https://qizekun.github.io/shapellm/">ShapeLLM</a> and <a href="https://moka-manipulation.github.io/">MoKA</a>. We thank the authors for their codebase.
      </p>
    </div>
  </footer>


</body>
</html>
