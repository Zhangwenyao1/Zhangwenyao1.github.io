<!DOCTYPE HTML>

<style>
  .toggle-text {
    cursor: pointer;
    display: inline-block;
    margin-right: 2px;
    color: #007BFF;
  }

  .toggle-text.active {
    font-weight: bold;
    text-decoration: none;
    color: #000000;
  }

  .content {
    display: none;
  }

  .content.active {
    display: block;
  }
</style>

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Wenyao Zhang</title>

  <meta name="author" content="Wenyao Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/icon.png"> -->
</head>


<body>
  <table
    style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:70%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Wenyao Zhang</name>
                  </p>
                  <p>
                    I am an Ph.D student at <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>, under the supervision of Prof. <a href="https://www.eitech.edu.cn/?tid=38&p=teacher"> Wenjun Zeng</a> and <a href="https://english.seiee.sjtu.edu.cn/english/detail/842_802.htm"> Xiaokang Yang </a>.
                    Previously, I obtained my master's degrees from <a href="https://www.sustech.edu.cn/">
                      Sourthern University of Science and Technoledge </a>, supervised by Prof. <a
                      href="https://www.sustech.edu.cn/en/faculties/jiazhenzhong.html">Zhenzhong Jia</a>.
                    And I received my bachelor's degree from <a href="https://www.bjtu.edu.cn/"> Beijing Jiao Tong University </a>, supervised by Prof. <a
                      href="https://faculty.bjtu.edu.cn/309/">Xin Zhang</a>.
                    I collaborate closely with Prof. <a href="https://www.eitech.edu.cn/?tid=40&p=teacher">Xin Jin</a>, <a href="https://ericyi.github.io/">Li Yi</a>, <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en"> Zhizheng Zhang</a> and <a
                      href="https://hughw19.github.io/">He Wang</a>.
                    <br>
                    I am currently a research intern at GalBot.
                  </p>
                  <p>
                    My research focuses on Robot Learning, Representation Learning and Multimodal Large Language
                    Models.
                  </p>

                  <p>
                    I am looking for collaborators to work on the following topics:
                    <ul>
                      <li>Vision-Language-Action Models</li>
                      <li>World Models</li>
                      <li>And other interesting topics in Robot Learning.</li>
                    </ul>
                  </p>
                  <p>
                    If you are interested in these topics, please feel free to contact me.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:wyzhang2022@gmail.com"> Email </a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?hl=en&user=w_Szx5MAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/Zhangwenyao1"> Github </a> &nbsp/&nbsp
                    <!-- <a href="Resume_Zekun_Qi.pdf" target="_blank"> CV </a> -->
                  </p>
                </td>
                <td style="padding:2.5%;width:30%;max-width:30%">
                  <img style="width:85%;max-width:85%" alt="profile photo" src="images/qzk.jpg">
                </td>
              </tr>
            </tbody>
          </table>
<!-- 
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                    <li style="margin: 5px;">
                      <b>2025-01:</b> One paper accepted to <a href="https://iclr.cc/Conferences/2025">ICLR
                        2025</a></a>.
                    </li>
                    <li style="margin: 5px;">
                      <b>2024-07:</b> One paper accepted to <a href="https://eccv.ecva.net/Conferences/2024">ECCV
                        2024</a> and one paper acccepted to <a href="https://2024.acmmm.org/">ACMMM 2024</a>.
                    </li>
                    <li style="margin: 5px;">
                      <b>2024-01:</b> One paper accepted to <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a> as
                      <font color="#E43F37">Spotlight</font> presentation.
                    </li>
                    <li style="margin: 5px;">
                      <b>2023-09:</b> One paper accepted to <a href="https://nips.cc/Conferences/2023">NeurIPS 2023</a>.
                    </li>
                    <li style="margin: 5px;">
                      <b>2023-04:</b> One paper accepted to <a href="https://icml.cc/Conferences/2023">ICML 2023</a>.
                    </li>
                    <li style="margin: 5px;">
                      <b>2023-01:</b> One paper accepted to <a href="https://iclr.cc/Conferences/2023">ICLR 2023</a>.
                    </li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table> -->

<!-- 
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    <heading>Publications</heading>
                  </p>
                  <p>
                    * indicates equal contribution &nbsp&nbsp&nbsp
                    <span id="txtSelected" class="toggle-text active">Show Selected</span>
                    /
                    <span id="txtDate" class="toggle-text">Show by Date</span>
                  </p>
                </td>
              </tr>
            </tbody>
          </table> -->

          <div id="contentSelected" class="content active">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <!-- This is the start of selected works.-->
                <tr>
                  <td style="padding:20px;width:30%;max-width:30%" align="center"> <img src="images/hybrid_depth.png"
                      alt="ICCV 2025" style="width:100%;max-width:100%"> 
                    </td>

                  <td width="75%" valign="center">
                    <papertitle>Hybrid-grained Feature Aggregation with Coare-to-fine Language Guidance for Self-supervised Monocular Depth Estimation</papertitle>
                    <br>
                    <strong>Wenyao Zhang</strong>*,
                    <a> Hongsi Liu </a>*, 
                    <a> Bohan Li </a>*,
                    <a href="https://jiaweihe.com/">Jiawei He</a>,
                    <a href="https://qizekun.github.io/">Zekun Qi</a>,
                    <a href="https://yunnanwang.github.io/">Yunnan Wang</a>,
                    <a> Shengyang Zhao </a>,
                    <a> Xinqiang Yu </a>,
                    <a href="https://wenjunzeng.github.io/"> Wenjun Zeng</a>
                    <a href="http://home.ustc.edu.cn/~jinxustc/">Xin Jin</a>,
                    <br>
                    <em>
                    <font color="#663399"><strong>International Conference on Computer Vision (ICCV 2025)</strong></font>
                    </em>
                    <br>
                    <a href="https://arxiv.org/abs/2506.03135">[arXiv]</a>
                    <a href="https://qizekun.github.io/omnispatial/">[Project Page]</a>
                    <a href="https://github.com/qizekun/OmniSpatial">[Code]</a>
                    <a href="https://huggingface.co/datasets/qizekun/OmniSpatial">[Huggingface]</a>
                    <br>
                    <p>Based on cognitive psychology, we introduce a comprehensive and complex spatial reasoning
                      benchmark, including 50 detailed categories and 1.5K manual labeled QA pairs.</p>
                  </td>
                </tr>


            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center"> <img src="images/DexVLG.png" alt="ICCV 2025"
                  style="width:100%;max-width:100%">
              </td>
            
              <td width="75%" valign="center">
                <papertitle>DexVLG: Dexterous Vision-Language-Grasp Model at Scale</papertitle>
                <br>
                <a href="https://jiaweihe.com/">Jiawei He</a>*,
                <a> Danshi Li</a>*,
                <a> Xinqiang Yu </a>*,
                <a href="https://qizekun.github.io/">Zekun Qi</a>*,
                <strong>Wenyao Zhang</strong>*,
                <a> Jiayi Chen</a>,
                <a> Zhaoxiang Zhang</a>,
                <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en">Zhizheng Zhang</a>,
                <a href="https://ericyi.github.io/">Li Yi</a>
                <a href="https://hughw19.github.io/">He Wang</a>,
                <br>
                <em>
                  <font color="#663399"><strong>International Conference on Computer Vision (ICCV 2025)</strong></font>
                </em>
                <br>
                <a href="https://arxiv.org/abs/2506.03135">[arXiv]</a>
                <a href="https://qizekun.github.io/omnispatial/">[Project Page]</a>
                <a href="https://github.com/qizekun/OmniSpatial">[Code]</a>
                <a href="https://huggingface.co/datasets/qizekun/OmniSpatial">[Huggingface]</a>
                <br>
                <p>Based on cognitive psychology, we introduce a comprehensive and complex spatial reasoning
                  benchmark, including 50 detailed categories and 1.5K manual labeled QA pairs.</p>
              </td>
            </tr>


                <tr>
                  <td style="padding:20px;width:30%;max-width:30%" align="center">
                    <video style="width:100%;max-width:100%" autoplay loop playsinline muted>
                      <source src="images/omnispatial25.mp4" type="video/mp4">
                    </video>
                  </td>
                  <td width="75%" valign="center">
                    <papertitle>OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language
                      Models</papertitle>
                    <br>
                    <a>Mengdi Jia</a>*,
                    <a href="https://qizekun.github.io/">Zekun Qi</a>*,
                    <a>Shaochen Zhang</a>,
                    <strong>Wenyao Zhang</strong>,
                    <a>Xinqiang Yu</a>,
                    <a href="https://jiaweihe.com/">Jiawei He</a>,
                    <a href="https://hughw19.github.io/">He Wang</a>,
                    <a href="https://ericyi.github.io/">Li Yi</a>

                    <br>
                    <em>arXiv preprint, 2025</em>
                    <br>
                    <a href="https://arxiv.org/abs/2506.03135">[arXiv]</a>
                    <a href="https://qizekun.github.io/omnispatial/">[Project Page]</a>
                    <a href="https://github.com/qizekun/OmniSpatial">[Code]</a>
                    <a href="https://huggingface.co/datasets/qizekun/OmniSpatial">[Huggingface]</a>
                    <br>
                    <p>Based on cognitive psychology, we introduce a comprehensive and complex spatial reasoning
                      benchmark, including 50 detailed categories and 1.5K manual labeled QA pairs.</p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:20px;width:30%;max-width:30%" align="center">
                    <video style="width:100%;max-width:100%" autoplay loop playsinline muted>
                      <source src="images/sofar25.mp4" type="video/mp4">
                    </video>
                  </td>
                  <td width="75%" valign="center">
                    <papertitle>SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation
                    </papertitle>
                    <br>
                    <a href="https://qizekun.github.io/">Zekun Qi</a>*,
                    <strong>Wenyao Zhang</strong>*,
                    <a href="https://selina2023.github.io/">Yufei Ding</a>*,
                    <a href="https://runpeidong.web.illinois.edu/">Runpei Dong</a>,
                    <a>Xinqiang Yu</a>,
                    <a>Jingwen Li</a>,
                    <a>Lingyun xu</a>,
                    <a href="https://baoyuli.github.io/">Baoyu Li</a>,
                    <a href="https://xialin-he.github.io/">Xialin He</a>,
                    <a href="https://github.com/Asterisci/">Guofan Fan</a>,
                    <a href="https://jzhzhang.github.io/">Jiazhao Zhang</a>,
                    <a href="https://jiaweihe.com/">Jiawei He</a>,
                    <a href="https://jiayuan-gu.github.io/">Jiayuan Gu</a>,
                    <a href="http://home.ustc.edu.cn/~jinxustc/">Xin Jin</a>,
                    <a href="https://group.iiis.tsinghua.edu.cn/~maks/">Kaisheng Ma</a>,
                    <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en">Zhizheng Zhang</a>,
                    <a href="https://hughw19.github.io/">He Wang</a>,
                    <a href="https://ericyi.github.io/">Li Yi</a>

                    <br>
                    <em>arXiv preprint, 2025</em>
                    <br>
                    <a href="https://arxiv.org/abs/2502.13143">[arXiv]</a>
                    <a href="https://qizekun.github.io/sofar/">[Project Page]</a>
                    <a href="https://github.com/qizekun/SoFar">[Code]</a>
                    <a
                      href="https://huggingface.co/collections/qizekun/sofar-67b511129d3146d28cea9920">[Huggingface]</a>
                    <br>
                    <p> We introduce the concept of semantic orientation, representing the object orientation condition
                      on open vocabulary language.</p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:20px;width:30%;max-width:30%" align="center"> <img src="images/nips-disco.png"
                      alt="NeurIPS 2024 Spotlight" style="width:100%;max-width:100%"> </td>
                  <td width="75%" valign="center">
                    <papertitle>Scene Graph Disentanglement and Composition for Generalizable Complex Image Generation</papertitle> <br>
                    <a href="https://yunnanwang.github.io/">Yunnan Wang</a>, <a href="https://ziqiangli.github.io/">Ziqiang
                      Li</a>, <strong>Wenyao Zhang</strong>, <a href="https://zequnzhang.github.io/">Zequn Zhang</a>, <a
                      href="https://baao.github.io/">Baao Xie</a>, <a href="https://xihuiliu.github.io/">Xihui Liu</a>, <a
                      href="https://wenjunzeng.github.io/">Wenjun Zeng</a>, <a href="https://xjin.org/">Xin Jin</a> <br> <em>
                      <font color="#663399"><strong>Annual Conference on Neural Information Processing Systems (NeurIPS 2024)</strong>
                      </font>
                    </em> <em>
                      <font color="red"><strong>Spotlight</strong></font>
                    </em> 
                    <br> 
                    <a href="https://neurips.cc/virtual/2024/poster/92965">[arXiv]</a>
                    <a href="https://neurips.cc/virtual/2024/poster/92965">[Code]</a>
                    <br>
                    <p>We propose a framework that disentangles scene graphs into semantic components and recomposes them to achieve
                      complex, generalizable image generation.</p>
                  </td>
                </tr>




                <tr>
                  <td style="padding:20px;width:30%;max-width:30%" align="center"> <img src="images/prompt.png"
                      alt="TMM" style="width:100%;max-width:100%"> </td>
                  <td width="75%" valign="center">
                    <papertitle>Unleash the Power of Vision-Language Models by Visual Attention Prompt and Multimodal Interaction</papertitle> <br>
                    <strong>Wenyao Zhang</strong>, Letian Wu , <a href="https://zequnzhang.github.io/">Zequn Zhang</a>, <a href="https://geekyutao.github.io//">Tao Yu</a>, <a
                      href="https://vision.sjtu.edu.cn/">Chao Ma</a>, <a href="https://xjin.org/">Xin Jin</a>, <a href="https://english.seiee.sjtu.edu.cn/english/detail/842_802.htm">Xiaokang Yang</a>, <a
                      href="https://wenjunzeng.github.io/">Wenjun Zeng</a> 
                    <br> 
                    <em> <font color="#663399"> <strong> IEEE Transactions on Multimedia (TMM 2024)</strong></font></em>
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10814093">[Paper]</a>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10814093">[Code]</a>
                    <br>
                    <p> We propose a framework that transfers VLMs to downstream tasks by designing visual prompts from an attention perspective
                    that reduces the transfer solution space. </p>
                  </td>
                </tr>






                <tr>
                  <td style="padding:20px;width:30%;max-width:30%" align="center">
                    <img src="images/close_loop.png" alt="ECCV 2024" style="width:100%;max-width:100%">
                  </td>
                  <td width="75%" valign="center">
                    <papertitle>Closed-Loop Unsupervised Representation Disentanglement with β-VAE Distillation and Diffusion
                      Probabilistic Feedback</papertitle>
                    <br>
                    Xin Jin, Bohan Li, Baao Xie, <strong>Wenyao Zhang</strong>, Jinming Liu, Ziqiang Li, Tao Yang, Wenjun Zeng
                    <br>
                    <em>
                    <strong> <font color="#663399"> European Conference on Computer Vision (ECCV 2024)</strong></font>
                    </em>
                    <br>
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-72995-9_16">[Paper]</a>
                    <a href="https://github.com/your-repo/CL-Dis">[Code]</a>
                    <br>
                    <p>
                      We introduce CL-Dis, a closed-loop unsupervised disentanglement framework that integrates β-VAE distillation with
                      diffusion-based feedback to learn semantically disentangled representations without labels.
                    </p>
                  </td>
                </tr>
                
                
                                
                <tr>
                  <td style="padding:20px;width:30%;max-width:30%" align="center">
                    <img src="images/occ.png" alt="ECCV 2024" style="width:100%;max-width:100%">
                  </td>
                  <td width="75%" valign="center">
                    <papertitle>Hierarchical Temporal Context Learning for Camera-Based Semantic Scene Completion</papertitle>
                    <br>
                    Bohan Li, Jiajun Deng, <strong>Wenyao Zhang</strong>, Zhujin Liang, Dalong Du, <a href="https://xjin.org/">Xin
                      Jin</a>, Wenjun Zeng
                    <br>
                    <em>
                      <font color="#663399"><strong>European Conference on Computer Vision (ECCV 2024)</strong></font>
                    </em>
                    <br>
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-73235-5_8">[Paper]</a>
                    <a href="https://github.com/Arlo0o/HTCL">[Code]</a>
                    <br>
                    <p>
                      We introduce HTCL, a hierarchical temporal context learning paradigm for camera-based 3D semantic scene
                      completion.
                    </p>
                  </td>
                </tr>


                <tr>
                  <td style="padding:20px;width:30%;max-width:30%" align="center"> <img src="images/rover.png" alt="TMM"
                      style="width:100%;max-width:100%"> </td>
                  <td width="75%" valign="center">
                    <papertitle>Predict the Rover Mobility Over Soft Terrain Using Articulated Wheeled Bevameter</papertitle> <br>
                    <strong>Wenyao Zhang</strong>, Shipeng Lyv , Feng Xue</a>, <a href="https://geekyutao.github.io//">Chen Yao</a>, <a
                      href="https://vision.sjtu.edu.cn/">Zheng Zhu</a>, <a href="https://xjin.org/">Zhenzhong Jia</a>
                    <br>
                    <em>
                      <font color="#663399"> <strong> IEEE Robotics and Automation Letters (RA-L 2022) </strong></font>
                    </em>
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/9906436/">[Paper]</a>
                    <a href="https://ieeexplore.ieee.org/abstract/document/9906436/">[Code]</a>
                    <br>
                    <p> We propose an on-board mobility prediction approach using an articulated wheeled bevameter that consists of a
                      force-controlled arm and an instrumented bevameter (with force and vision sensors) as its end-effector. </p>
                  </td>
                </tr>

              </tbody>
            </table>
          </div>




         
<!-- 
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Honors and Awards</heading>
                  <p>
                    <li style="margin: 5px;"> 2024 National Scholarship & Xiaomi Special Scholarship, Xi’an Jiaotong
                      University</li>
                    <li style="margin: 5px;"> 2022 Outstanding Graduate, Xi’an Jiaotong University</li>
                    <li style="margin: 5px;"> 2021 <a href="https://mp.weixin.qq.com/s/nSirUtlo1j87JKXvJ9BttA"> Annual
                        Spiritual Civilization Award </a>, Xi’an Jiaotong University</li>
                    <li style="margin: 5px;"> 2020 <a href="https://mp.weixin.qq.com/s/2J32r3O4VsJnre3ckPPXrQ"> National
                        runner-up </a> of the China Undergraduate Physics Tournament (CUPT) as the team leader </li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table> -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    <a href="https://jonbarron.info/">Website Template</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

      </tr>
    </tbody>
  </table>

  <p>
  <div class="center" id="clustrmaps-widget" style="width:8%">
    <script type="text/javascript" id="clstr_globe"
      src="//clustrmaps.com/globe.js?d=kSH52UUctXHgyZTvsD38_H_styzXoRq6qG_cg4tttWI">& copy; Zekun Qi | Last updated: Aug 6, 2023</script>
  </div>
  <br>
  <div style="text-align:center;"> &copy; Zekun Qi | Last updated: Feb 19, 2025 </div>
  </p>

  <script>
    document.getElementById('txtSelected').addEventListener('click', function () {
      toggleContent('Selected');
    });

    document.getElementById('txtDate').addEventListener('click', function () {
      toggleContent('Date');
    });

    function toggleContent(type) {
      document.querySelectorAll('.toggle-text').forEach(text => {
        text.classList.remove('active');
      });
      document.querySelectorAll('.content').forEach(content => {
        content.classList.remove('active');
      });

      document.getElementById('txt' + type).classList.add('active');
      document.getElementById('content' + type).classList.add('active');
    }
  </script>

</body>

</html>